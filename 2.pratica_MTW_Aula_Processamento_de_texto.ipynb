{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valeriafigueredo-data/Mineracao_de_Texto_e_na_Web/blob/main/2.pratica_MTW_Aula_Processamento_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Segundo Trabalho da Disciplina Mineração de Texto e na Web**\n",
        "\n",
        "**GRUPO**\n",
        "\n",
        "*   **André Felipe Bezerra de Souza Leão (afbsl@cesar.school)**\n",
        "*   **Manuela de Lacerda Bezerra Carvalho (mlbc@cesar.school)**\n",
        "*   **Valéria Cristina Andrade Rodrigues de Figueredo (vcarf@cesar.school)**"
      ],
      "metadata": {
        "id": "Z70i9AaT02aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Segmentação de sentenças**\n",
        "\n",
        "NLTK -> Lib em python que permite vários processamentos de texto, entre eles a segmentação de sentenças.\n",
        "- Para instalar a lib\n",
        "\n",
        "```\n",
        "pip install nltk\n",
        "```\n",
        "- É necessário baixar o ``` pickle ``` com os tokens\n",
        "```\n",
        "nltk.download('punkt')\n",
        "```  \n",
        "\n"
      ],
      "metadata": {
        "id": "pjoyVIeR_ivb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPlucbJDAqeV",
        "outputId": "dda077b6-89c1-4960-c950-81ca0f32ef2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wyDdcExFwCCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "text = \"This is a English sample of the lib NLTK. NLTK is a lib for NLP.\"\n",
        "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "result = sentence_tokenizer.tokenize(text)\n",
        "print(len(result))\n",
        "\n",
        "for n, sentence in enumerate(result):\n",
        "  print(f\"{n}: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sQ2gzvH_pZX",
        "outputId": "cbd6af81-d13f-4710-9f27-1e3cc1b2df05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0: This is a English sample of the lib NLTK.\n",
            "1: NLTK is a lib for NLP.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4MnlHpBwkcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gX_vT9EUwFyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_pt= \"Eu tenho uma gata. Essa gata dorme no meu colo! mas eu tenho outra gata que não gosta de colo\"\n",
        "sentence_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
        "result = sentence_tokenizer.tokenize(text_pt)\n",
        "print(len(result))\n",
        "\n",
        "for n, sentence in enumerate(result):\n",
        "  print(f\"{n}: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vOwMIRYCoNc",
        "outputId": "fe0763f2-d42d-4850-e473-e1a02d93e569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0: Eu tenho uma gata.\n",
            "1: Essa gata dorme no meu colo!\n",
            "2: mas eu tenho outra gata que não gosta de colo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenização de texto**"
      ],
      "metadata": {
        "id": "o7kKag13HbNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "text = \"Eu gosto de café sem açúcar, mas se o café estiver forte eu ponho açúcar. Em nossa casa as pessoas costumão tomar café com bolachas\"\n",
        "[s.lower() for s in tokenizer.tokenize(text)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuYLgfu2Hp6C",
        "outputId": "7c34e4aa-b954-4e87-92c3-64dd11a144b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eu',\n",
              " 'gosto',\n",
              " 'de',\n",
              " 'café',\n",
              " 'sem',\n",
              " 'açúcar',\n",
              " 'mas',\n",
              " 'se',\n",
              " 'o',\n",
              " 'café',\n",
              " 'estiver',\n",
              " 'forte',\n",
              " 'eu',\n",
              " 'ponho',\n",
              " 'açúcar',\n",
              " 'em',\n",
              " 'nossa',\n",
              " 'casa',\n",
              " 'as',\n",
              " 'pessoas',\n",
              " 'costumão',\n",
              " 'tomar',\n",
              " 'café',\n",
              " 'com',\n",
              " 'bolachas']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remoção de StopWords**\n",
        "\n",
        "-É necessário ter uma lista de stopwords, no NLTK basta baixar essa lista:\n",
        "```nltk.download('stopwords')```\n"
      ],
      "metadata": {
        "id": "15A9lOs-Mdyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ecInXB2NY0_",
        "outputId": "a11b19b6-ebe8-4b4d-9cea-c3fad95e5644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens_v0 = [s.lower() for s in tokenizer.tokenize(text)]\n",
        "stop_words = stopwords.words('portuguese')\n",
        "tokens_v1 = [token for token in tokens_v0 if not token in stop_words]\n",
        "\n",
        "print(tokens_v0)\n",
        "print(tokens_v1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLnTdrq8MjZ7",
        "outputId": "980282da-4f27-4fe3-cc0d-ea75f0e73182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'gosto', 'de', 'café', 'sem', 'açúcar', 'mas', 'se', 'o', 'café', 'estiver', 'forte', 'eu', 'ponho', 'açúcar', 'em', 'nossa', 'casa', 'as', 'pessoas', 'costumão', 'tomar', 'café', 'com', 'bolachas']\n",
            "['gosto', 'café', 'açúcar', 'café', 'forte', 'ponho', 'açúcar', 'casa', 'pessoas', 'costumão', 'tomar', 'café', 'bolachas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words.remove(\"sem\")\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trEI1g2Gy1BK",
        "outputId": "76104d5e-c0f2-4a02-db1b-31d1bfa68e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'fui', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'não', 'nas', 'nem', 'no', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'são', 'se', 'seja', 'sejam', 'sejamos', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'seu', 'seus', 'só', 'somos', 'sou', 'sua', 'suas', 'também', 'te', 'tem', 'tém', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terá', 'terão', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tu', 'tua', 'tuas', 'um', 'uma', 'você', 'vocês', 'vos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens_v0 = [s.lower() for s in tokenizer.tokenize(text)]\n",
        "# stop_words = stopwords.words('portuguese')\n",
        "tokens_v1 = [token for token in tokens_v0 if not token in stop_words]\n",
        "\n",
        "print(tokens_v0)\n",
        "print(tokens_v1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Bhcm6jzJi7",
        "outputId": "4ddca0a8-80ff-4920-d03e-f68171416bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'gosto', 'de', 'café', 'sem', 'açúcar', 'mas', 'se', 'o', 'café', 'estiver', 'forte', 'eu', 'ponho', 'açúcar', 'em', 'nossa', 'casa', 'as', 'pessoas', 'costumão', 'tomar', 'café', 'com', 'bolachas']\n",
            "['gosto', 'café', 'sem', 'açúcar', 'café', 'forte', 'ponho', 'açúcar', 'casa', 'pessoas', 'costumão', 'tomar', 'café', 'bolachas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "6bBaKJBTQMdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.test.portuguese_en_fixt import setup_module\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI4TWOvT9hVd",
        "outputId": "825237ae-2534-4bd9-c99f-39b869ad3251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "lemma_words = [wordnet_lemmatizer.lemmatize(word) for word in tokens_v0]\n",
        "print(tokens_v0)\n",
        "print(lemma_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHBQt1F9S9wb",
        "outputId": "1d85546d-5ab6-450c-db30-9c3f1c5a6a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'gosto', 'de', 'café', 'sem', 'açúcar', 'mas', 'se', 'o', 'café', 'estiver', 'forte', 'eu', 'ponho', 'açúcar', 'em', 'nossa', 'casa', 'as', 'pessoas', 'costumão', 'tomar', 'café', 'com', 'bolachas']\n",
            "['eu', 'gosto', 'de', 'café', 'sem', 'açúcar', 'ma', 'se', 'o', 'café', 'estiver', 'forte', 'eu', 'ponho', 'açúcar', 'em', 'nossa', 'casa', 'a', 'pessoas', 'costumão', 'tomar', 'café', 'com', 'bolachas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming**"
      ],
      "metadata": {
        "id": "WztxTvKlUUbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('rslp')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import RSLPStemmer\n",
        "rslp_stemmer = RSLPStemmer()\n",
        "porter_stemmer = PorterStemmer()\n",
        "stem_words = [porter_stemmer.stem(word) for word in tokens_v0]\n",
        "stem_words2 = [rslp_stemmer.stem(word) for word in tokens_v0]\n",
        "\n",
        "print(tokens_v0)\n",
        "print(stem_words)\n",
        "print(stem_words2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBqiP2XnT8on",
        "outputId": "a9325adc-1080-40cf-ff1a-7d357541a0c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'gosto', 'de', 'café', 'sem', 'açúcar', 'mas', 'se', 'o', 'café', 'estiver', 'forte', 'eu', 'ponho', 'açúcar', 'em', 'nossa', 'casa', 'as', 'pessoas', 'costumão', 'tomar', 'café', 'com', 'bolachas']\n",
            "['eu', 'gosto', 'de', 'café', 'sem', 'açúcar', 'ma', 'se', 'o', 'café', 'estiv', 'fort', 'eu', 'ponho', 'açúcar', 'em', 'nossa', 'casa', 'as', 'pessoa', 'costumão', 'tomar', 'café', 'com', 'bolacha']\n",
            "['eu', 'gost', 'de', 'café', 'sem', 'açúc', 'mas', 'se', 'o', 'café', 'estiv', 'fort', 'eu', 'ponh', 'açúc', 'em', 'noss', 'cas', 'as', 'pesso', 'costum', 'tom', 'café', 'com', 'bolach']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming Vs Lemmatization"
      ],
      "metadata": {
        "id": "jo0NRjsRYOuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_sample= ['amigos', 'amigas', 'amizade', 'carreira', 'carreiras', 'tempestade']\n",
        "lemma_words = [wordnet_lemmatizer.lemmatize(word) for word in words_sample]\n",
        "stem_words = [porter_stemmer.stem(word) for word in words_sample]\n",
        "\n",
        "print(words_sample)\n",
        "print(lemma_words)\n",
        "print(stem_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QomryHJeUabo",
        "outputId": "b501f76f-f394-498e-8950-b6dae863a16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amigos', 'amigas', 'amizade', 'carreira', 'carreiras', 'tempestade']\n",
            "['amigo', 'amigas', 'amizade', 'carreira', 'carreiras', 'tempestade']\n",
            "['amigo', 'amiga', 'amizad', 'carreira', 'carreira', 'tempestad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZbeVpbUyjen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extração de características\n",
        "\n",
        "## Part-of-speech\n",
        "\n",
        "Em inglês:\n",
        "1. CC - Coordinating conjunction\n",
        "2. CD - Cardinal number\n",
        "3. DT - Determiner\n",
        "4. EX - Existential there\n",
        "5. FW - Foreign word\n",
        "6. IN - Preposition or subordinating conjunction\n",
        "7. JJ - Adjective\n",
        "8. JJR - Adjective, comparative\n",
        "9. JJS - Adjective, superlative\n",
        "10. LS - List item marker\n",
        "11. MD - Modal\n",
        "12. NN - Noun, singular or mass\n",
        "13. NNS - Noun, plural\n",
        "14. NNP - Proper noun, singular\n",
        "15. NNPS - Proper noun, plural\n",
        "16. PDT - Predeterminer\n",
        "17. POS - Possessive ending\n",
        "18. PRP - Personal pronoun\n",
        "19. PRP - \tPossessive pronoun\n",
        "20. RB - Adverb\n",
        "21. RBR - Adverb, comparative\n",
        "22. RBS - Adverb, superlative\n",
        "23. RP - Particle\n",
        "24. SYM - Symbol\n",
        "25. TO - to\n",
        "26. UH - Interjection\n",
        "27. VB - Verb, base form\n",
        "28. VBD - Verb, past tense\n",
        "29. VBG - Verb, gerund or present participle\n",
        "30. VBN - Verb, past participle\n",
        "31. VBP - Verb, non-3rd person singular present\n",
        "32. VBZ - Verb, 3rd person singular present\n",
        "33. WDT - Wh-determiner\n",
        "34. WP - Wh-pronoun\n",
        "35. WP - \tPossessive wh-pronoun\n",
        "36. WRB - Wh-adverb"
      ],
      "metadata": {
        "id": "0pED1KUd89y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCMq9xcT9WZ4",
        "outputId": "d641193f-2ff4-42f9-a5ef-571c488cdecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text =\"Happiness can be found, even in the darkest of times, if one only remembers to turn on the light.\"\n",
        "tokens = [s.lower() for s in tokenizer.tokenize(text)]\n",
        "nltk.pos_tag(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpQYy3QR9Mfu",
        "outputId": "f3d6e0c5-eb2c-4392-f13f-f290417c8d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('happiness', 'NN'),\n",
              " ('can', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('found', 'VBN'),\n",
              " ('even', 'RB'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('darkest', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('times', 'NNS'),\n",
              " ('if', 'IN'),\n",
              " ('one', 'CD'),\n",
              " ('only', 'RB'),\n",
              " ('remembers', 'VBZ'),\n",
              " ('to', 'TO'),\n",
              " ('turn', 'VB'),\n",
              " ('on', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('light', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extração de características TF\n"
      ],
      "metadata": {
        "id": "Ipa7ws7vJVg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_v0)\n",
        "print(tokens_v1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtbiVOkrJZRw",
        "outputId": "714f8082-0c83-4b6e-e22d-5ae06096b933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'gosto', 'de', 'café', 'sem', 'açúcar', 'mas', 'se', 'o', 'café', 'estiver', 'forte', 'eu', 'ponho', 'açúcar', 'em', 'nossa', 'casa', 'as', 'pessoas', 'costumão', 'tomar', 'café', 'com', 'bolachas']\n",
            "['gosto', 'café', 'sem', 'açúcar', 'café', 'forte', 'ponho', 'açúcar', 'casa', 'pessoas', 'costumão', 'tomar', 'café', 'bolachas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(data=[np.zeros(11)], columns=list(set(tokens_v1)), index=[\"N\",\"TF\"])\n",
        "quant_token = len(list(set(tokens_v1)))\n",
        "\n",
        "for word in tokens_v0:\n",
        "  if word in df.keys():\n",
        "    df[word][\"N\"] += 1\n",
        "\n",
        "for token in df.keys():\n",
        "  df[token][\"TF\"] = df[token][\"N\"]/quant_token\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "uTR8LEL1KA2V",
        "outputId": "a636d97d-166e-4b4b-ba59-69c2c722f42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-97e5b416e1a8>:9: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df[word][\"N\"] += 1\n",
            "<ipython-input-65-97e5b416e1a8>:12: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  df[token][\"TF\"] = df[token][\"N\"]/quant_token\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       forte   pessoas      café     tomar       sem     ponho  costumão  \\\n",
              "N   1.000000  1.000000  3.000000  1.000000  1.000000  1.000000  1.000000   \n",
              "TF  0.090909  0.090909  0.272727  0.090909  0.090909  0.090909  0.090909   \n",
              "\n",
              "       gosto    açúcar      casa  bolachas  \n",
              "N   1.000000  2.000000  1.000000  1.000000  \n",
              "TF  0.090909  0.181818  0.090909  0.090909  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ec79b9a-052a-454e-b34b-ebf266723b60\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>forte</th>\n",
              "      <th>pessoas</th>\n",
              "      <th>café</th>\n",
              "      <th>tomar</th>\n",
              "      <th>sem</th>\n",
              "      <th>ponho</th>\n",
              "      <th>costumão</th>\n",
              "      <th>gosto</th>\n",
              "      <th>açúcar</th>\n",
              "      <th>casa</th>\n",
              "      <th>bolachas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>N</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TF</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ec79b9a-052a-454e-b34b-ebf266723b60')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4ec79b9a-052a-454e-b34b-ebf266723b60 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4ec79b9a-052a-454e-b34b-ebf266723b60');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c07cc3c0-e312-439c-bfce-ffa36ec5b55b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c07cc3c0-e312-439c-bfce-ffa36ec5b55b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c07cc3c0-e312-439c-bfce-ffa36ec5b55b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2bedf91d-73a8-4b4f-9172-439364660a08\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2bedf91d-73a8-4b4f-9172-439364660a08 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"forte\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pessoas\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caf\\u00e9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.928473039599675,\n        \"min\": 0.2727272727272727,\n        \"max\": 3.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2727272727272727,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tomar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sem\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ponho\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"costum\\u00e3o\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gosto\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"a\\u00e7\\u00facar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.28564869306645,\n        \"min\": 0.18181818181818182,\n        \"max\": 2.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.18181818181818182,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"casa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bolachas\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.642824346533225,\n        \"min\": 0.09090909090909091,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09090909090909091,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prática:\n",
        "\n",
        "1. Rode o código da célula a baixo para gerar o texto base\n",
        "2. Faça uma primeira exploração dos dados\n",
        " - Do que são esses dados?\n",
        " - Que tipo de \"sujeiras\" tem nele?\n",
        " - Quantos dados temos?\n",
        "3. Limpe os dados e os separe em sentenças\n",
        "4. Pra cada sentença faça:\n",
        "  - Tokenização\n",
        "  - Remoção de stopwords usando a lista da lib NLTK (reflita se tem alguma palavra que você colocaria/removeria da lista de stop words, qual seria?)\n",
        "5. Por fim, compare a Lemmatization com Stemming de cada frase.\n"
      ],
      "metadata": {
        "id": "jNQ3RzS_YUr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('mac_morpho')\n",
        "sents = nltk.corpus.mac_morpho.sents()\n",
        "text = \"<p>\"\n",
        "for s in sents[:10]:\n",
        "  text += \" \".join(s)\n",
        "  text += \".</p> |<p>\"\n",
        "\n",
        "print(len(text))\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "lDBH0yHhbg5P",
        "outputId": "36c37065-2612-4e82-9193-5dcf8244cf7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Package mac_morpho is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1157\n",
            "<p>Jersei atinge média de Cr$ 1,4 milhão em a venda de a Pinhal em São Paulo.</p> |<p>Programe sua viagem a a Exposição Nacional do Zebu , que começa dia 25.</p> |<p>Safra recorde e disponibilidade de crédito ativam vendas de máquinas agrícolas.</p> |<p>A degradação de as terras por o mau uso de os solos avança em o.</p> |<p>A desertificação tornou crítica a produtividade de 52 mil km² em a região.</p> |<p>Em o dia 15 , Dia da Conservação do Solo , o único fato a festejar pode ser a Convenção Internacional sobre Desertificação.</p> |<p>A produção brasileira de pintos de corte totalizou , em fevereiro último , 166 milhões , volume 6,79 % superior a o registrado em fevereiro de 93 , segundo dados de a Associação Brasileira dos Produtores de Pinto de Corte ( Apinco ).</p> |<p>A Apinco destaca em seu boletim mensal que o setor avícola está otimista com o atual programa de estabilização econômica.</p> |<p>\" A melhoria de o padrão aquisitivo resulta em maior demanda de carne de frango \".</p>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Exploração dos dados**"
      ],
      "metadata": {
        "id": "JyGT6UUHipVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Contexto: Texto com formato de Manchetes jornalísticas.\n",
        "*   As sujeiras são Tags html, problemas de concordância nominal que deixa a leitura confusa e sem clareza, a repetição de letras, tal qual \"a a\", palavras sem sentido para o significado da frase ( A degradação das terras \"avança em o\" - avança onde?, como não tem complemento, foi removido), e remoção de aspas, pois as mesmas não foram abertas nas frases que as continham.\n",
        "*   Quantidade de dados antes da limpeza: 206, considerando cada palavra, caractere, Tag HTML\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fb4GuUjHxyMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "-BizF7sm55VH",
        "outputId": "78045755-b6e3-4ad0-81fb-7dc4b49f49f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Limpeza de dados e separação em sentenças**"
      ],
      "metadata": {
        "id": "bqXQrxVZ3I4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text_clean = re.sub(r'\\<.*?\\>', '', text)\n",
        "text_clean = text_clean.replace('|', '')\n",
        "text_clean = text_clean.replace(' em a ', ' na ')\n",
        "text_clean = text_clean.replace(' de a ', ' da ')\n",
        "text_clean = text_clean.replace(' a a ', ' à ')\n",
        "text_clean = text_clean.replace(' de as ', ' das ')\n",
        "text_clean = text_clean.replace(' de os ', ' dos ')\n",
        "text_clean = text_clean.replace('Em o', 'No')\n",
        "text_clean = text_clean.replace(' a o ', ' ao ')\n",
        "text_clean = text_clean.replace(' de o ', ' do ')\n",
        "text_clean = text_clean.replace(' em o ', ' no ')\n",
        "text_clean = text_clean.replace(' em o.', '.')\n",
        "text_clean = text_clean.replace(' por o ', ' pelo ')\n",
        "text_clean = text_clean.replace(' \" ', ' ')\n",
        "text_clean = text_clean.replace(' \"', ' ')\n",
        "text_clean = text_clean.replace('da Pinhal', 'de Pinhal')\n",
        "text_clean = text_clean.replace('São Paulo', 'SP')\n",
        "\n",
        "print(text)\n",
        "print(text_clean)"
      ],
      "metadata": {
        "id": "RROs0AByEUgl",
        "outputId": "77bdf415-86b8-45bc-86ee-cd85aa4e9706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<p>Jersei atinge média de Cr$ 1,4 milhão em a venda de a Pinhal em São Paulo.</p> |<p>Programe sua viagem a a Exposição Nacional do Zebu , que começa dia 25.</p> |<p>Safra recorde e disponibilidade de crédito ativam vendas de máquinas agrícolas.</p> |<p>A degradação de as terras por o mau uso de os solos avança em o.</p> |<p>A desertificação tornou crítica a produtividade de 52 mil km² em a região.</p> |<p>Em o dia 15 , Dia da Conservação do Solo , o único fato a festejar pode ser a Convenção Internacional sobre Desertificação.</p> |<p>A produção brasileira de pintos de corte totalizou , em fevereiro último , 166 milhões , volume 6,79 % superior a o registrado em fevereiro de 93 , segundo dados de a Associação Brasileira dos Produtores de Pinto de Corte ( Apinco ).</p> |<p>A Apinco destaca em seu boletim mensal que o setor avícola está otimista com o atual programa de estabilização econômica.</p> |<p>\" A melhoria de o padrão aquisitivo resulta em maior demanda de carne de frango \".</p> |<p>O secretário de a agricultura paulista , Roberto Rodrigues , aprovou o pacote de o trigo , anunciado em o final de março por o governo federal.</p> |<p>\n",
            "Jersei atinge média de Cr$ 1,4 milhão na venda de Pinhal em SP. Programe sua viagem à Exposição Nacional do Zebu , que começa dia 25. Safra recorde e disponibilidade de crédito ativam vendas de máquinas agrícolas. A degradação das terras pelo mau uso dos solos avança. A desertificação tornou crítica a produtividade de 52 mil km² na região. No dia 15 , Dia da Conservação do Solo , o único fato a festejar pode ser a Convenção Internacional sobre Desertificação. A produção brasileira de pintos de corte totalizou , em fevereiro último , 166 milhões , volume 6,79 % superior ao registrado em fevereiro de 93 , segundo dados da Associação Brasileira dos Produtores de Pinto de Corte ( Apinco ). A Apinco destaca em seu boletim mensal que o setor avícola está otimista com o atual programa de estabilização econômica. A melhoria do padrão aquisitivo resulta em maior demanda de carne de frango . O secretário da agricultura paulista , Roberto Rodrigues , aprovou o pacote do trigo , anunciado no final de março pelo governo federal. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.tokenize(text_clean))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLxYXA42XfC4",
        "outputId": "ab8156c2-5bf0-4097-c5f6-a630edc6df46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após a limpeza dos dados, com a remoção da sujeira - tags HTLM, correção dos erros de concordância nominal, transformação de letras repetidas como \"a a\" para à, remoção de palavras sem sentido para o significado da frase (  A degradação das terras \"avança em o\" - avança onde?, como não tem complemento, foi removido) e remoção de aspas,- os dados limpos ficaram com 167 tokens.\n",
        "\n",
        "OBS: A palavra São Paulo estava correta, gramaticalmente falando. Entretanto, foi colocado SP para que não ocorra a remoção de \"são\" durante o processo de remoção de stopwords e a frase fique sem sentido."
      ],
      "metadata": {
        "id": "3bV-ba8QXkD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entence_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
        "sentences = sentence_tokenizer.tokenize(text_clean)\n",
        "print(len(sentences))\n",
        "\n",
        "for n, sentence in enumerate(sentences):\n",
        "  print(f\"{n}: {sentence}\")"
      ],
      "metadata": {
        "id": "Hiul7sRX8b5r",
        "outputId": "f2222564-a5b9-42ff-ffff-d093c9b87c36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "0: Jersei atinge média de Cr$ 1,4 milhão na venda de Pinhal em SP.\n",
            "1: Programe sua viagem à Exposição Nacional do Zebu , que começa dia 25.\n",
            "2: Safra recorde e disponibilidade de crédito ativam vendas de máquinas agrícolas.\n",
            "3: A degradação das terras pelo mau uso dos solos avança.\n",
            "4: A desertificação tornou crítica a produtividade de 52 mil km² na região.\n",
            "5: No dia 15 , Dia da Conservação do Solo , o único fato a festejar pode ser a Convenção Internacional sobre Desertificação.\n",
            "6: A produção brasileira de pintos de corte totalizou , em fevereiro último , 166 milhões , volume 6,79 % superior ao registrado em fevereiro de 93 , segundo dados da Associação Brasileira dos Produtores de Pinto de Corte ( Apinco ).\n",
            "7: A Apinco destaca em seu boletim mensal que o setor avícola está otimista com o atual programa de estabilização econômica.\n",
            "8: A melhoria do padrão aquisitivo resulta em maior demanda de carne de frango .\n",
            "9: O secretário da agricultura paulista , Roberto Rodrigues , aprovou o pacote do trigo , anunciado no final de março pelo governo federal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1 Tokenização**"
      ],
      "metadata": {
        "id": "WE8-d-giJ0H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [\n",
        "    [s.lower() for s in tokenizer.tokenize(sentence)]\n",
        "    for sentence in sentences\n",
        "]\n",
        "\n",
        "for n, sentence in enumerate(tokenized_sentences):\n",
        "  print(f\"{n}: {sentence}\")"
      ],
      "metadata": {
        "id": "ynjD9bZW3GyC",
        "outputId": "4af38e99-8ec1-4c0e-ed05-f4a3ad212625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: ['jersei', 'atinge', 'média', 'de', 'cr', '1', '4', 'milhão', 'na', 'venda', 'de', 'pinhal', 'em', 'sp']\n",
            "1: ['programe', 'sua', 'viagem', 'à', 'exposição', 'nacional', 'do', 'zebu', 'que', 'começa', 'dia', '25']\n",
            "2: ['safra', 'recorde', 'e', 'disponibilidade', 'de', 'crédito', 'ativam', 'vendas', 'de', 'máquinas', 'agrícolas']\n",
            "3: ['a', 'degradação', 'das', 'terras', 'pelo', 'mau', 'uso', 'dos', 'solos', 'avança']\n",
            "4: ['a', 'desertificação', 'tornou', 'crítica', 'a', 'produtividade', 'de', '52', 'mil', 'km²', 'na', 'região']\n",
            "5: ['no', 'dia', '15', 'dia', 'da', 'conservação', 'do', 'solo', 'o', 'único', 'fato', 'a', 'festejar', 'pode', 'ser', 'a', 'convenção', 'internacional', 'sobre', 'desertificação']\n",
            "6: ['a', 'produção', 'brasileira', 'de', 'pintos', 'de', 'corte', 'totalizou', 'em', 'fevereiro', 'último', '166', 'milhões', 'volume', '6', '79', 'superior', 'ao', 'registrado', 'em', 'fevereiro', 'de', '93', 'segundo', 'dados', 'da', 'associação', 'brasileira', 'dos', 'produtores', 'de', 'pinto', 'de', 'corte', 'apinco']\n",
            "7: ['a', 'apinco', 'destaca', 'em', 'seu', 'boletim', 'mensal', 'que', 'o', 'setor', 'avícola', 'está', 'otimista', 'com', 'o', 'atual', 'programa', 'de', 'estabilização', 'econômica']\n",
            "8: ['a', 'melhoria', 'do', 'padrão', 'aquisitivo', 'resulta', 'em', 'maior', 'demanda', 'de', 'carne', 'de', 'frango']\n",
            "9: ['o', 'secretário', 'da', 'agricultura', 'paulista', 'roberto', 'rodrigues', 'aprovou', 'o', 'pacote', 'do', 'trigo', 'anunciado', 'no', 'final', 'de', 'março', 'pelo', 'governo', 'federal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.2 Remoção de stopwords**"
      ],
      "metadata": {
        "id": "J4iuz2w_NQUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "\n",
        "filtered_sentences = [\n",
        "    [token for token in sentence if token not in stop_words]\n",
        "    for sentence in tokenized_sentences\n",
        "]\n",
        "\n",
        "for n, sentence in enumerate(filtered_sentences):\n",
        "  print(f\"{n}: {sentence}\")"
      ],
      "metadata": {
        "id": "sNzBw6wfNLWY",
        "outputId": "235e1a9f-f935-4746-e056-a03031237a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: ['jersei', 'atinge', 'média', 'cr', '1', '4', 'milhão', 'venda', 'pinhal', 'sp']\n",
            "1: ['programe', 'viagem', 'exposição', 'nacional', 'zebu', 'começa', 'dia', '25']\n",
            "2: ['safra', 'recorde', 'disponibilidade', 'crédito', 'ativam', 'vendas', 'máquinas', 'agrícolas']\n",
            "3: ['degradação', 'terras', 'mau', 'uso', 'solos', 'avança']\n",
            "4: ['desertificação', 'tornou', 'crítica', 'produtividade', '52', 'mil', 'km²', 'região']\n",
            "5: ['dia', '15', 'dia', 'conservação', 'solo', 'único', 'fato', 'festejar', 'pode', 'convenção', 'internacional', 'sobre', 'desertificação']\n",
            "6: ['produção', 'brasileira', 'pintos', 'corte', 'totalizou', 'fevereiro', 'último', '166', 'milhões', 'volume', '6', '79', 'superior', 'registrado', 'fevereiro', '93', 'segundo', 'dados', 'associação', 'brasileira', 'produtores', 'pinto', 'corte', 'apinco']\n",
            "7: ['apinco', 'destaca', 'boletim', 'mensal', 'setor', 'avícola', 'otimista', 'atual', 'programa', 'estabilização', 'econômica']\n",
            "8: ['melhoria', 'padrão', 'aquisitivo', 'resulta', 'maior', 'demanda', 'carne', 'frango']\n",
            "9: ['secretário', 'agricultura', 'paulista', 'roberto', 'rodrigues', 'aprovou', 'pacote', 'trigo', 'anunciado', 'final', 'março', 'governo', 'federal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foi realizada a remoção de stopwords usando a lista da lib NLTK. Portanto, foi observado que seria removida a palavra são, de São Paulo. Isso afetaria a clareza e compreensão da frase. Assim, para isso não ocorrer, essa palavra - \"são\" deveria ser removida da lista de stop words. Entretanto, tal problema já foi solucionado na etapa de limpeza de dados, transformando São Paulo em SP."
      ],
      "metadata": {
        "id": "KxNk0l-tyNEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Lemmatization X Stemming**\n"
      ],
      "metadata": {
        "id": "kmHOcQgfOnuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_sentences = [\n",
        "    [wordnet_lemmatizer.lemmatize(word) for word in sentence]\n",
        "    for sentence in filtered_sentences\n",
        "]\n",
        "\n",
        "stem_sentences = [\n",
        "    [porter_stemmer.stem(word) for word in sentence]\n",
        "    for sentence in filtered_sentences\n",
        "]\n",
        "\n",
        "lists = [lemma_sentences, stem_sentences]\n",
        "\n",
        "for lst in lists:\n",
        "    print(lst)"
      ],
      "metadata": {
        "id": "EuKxG713OnPc",
        "outputId": "91cd31ed-b96c-42eb-fc27-fd6a175d7ccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['jersei', 'atinge', 'média', 'cr', '1', '4', 'milhão', 'venda', 'pinhal', 'sp'], ['programe', 'viagem', 'exposição', 'nacional', 'zebu', 'começa', 'dia', '25'], ['safra', 'recorde', 'disponibilidade', 'crédito', 'ativam', 'vendas', 'máquinas', 'agrícolas'], ['degradação', 'terras', 'mau', 'uso', 'solo', 'avança'], ['desertificação', 'tornou', 'crítica', 'produtividade', '52', 'mil', 'km²', 'região'], ['dia', '15', 'dia', 'conservação', 'solo', 'único', 'fato', 'festejar', 'pode', 'convenção', 'internacional', 'sobre', 'desertificação'], ['produção', 'brasileira', 'pinto', 'corte', 'totalizou', 'fevereiro', 'último', '166', 'milhões', 'volume', '6', '79', 'superior', 'registrado', 'fevereiro', '93', 'segundo', 'dado', 'associação', 'brasileira', 'produtores', 'pinto', 'corte', 'apinco'], ['apinco', 'destaca', 'boletim', 'mensal', 'setor', 'avícola', 'otimista', 'atual', 'programa', 'estabilização', 'econômica'], ['melhoria', 'padrão', 'aquisitivo', 'resulta', 'maior', 'demanda', 'carne', 'frango'], ['secretário', 'agricultura', 'paulista', 'roberto', 'rodrigues', 'aprovou', 'pacote', 'trigo', 'anunciado', 'final', 'março', 'governo', 'federal']]\n",
            "[['jersei', 'ating', 'média', 'cr', '1', '4', 'milhão', 'venda', 'pinhal', 'sp'], ['program', 'viagem', 'exposição', 'nacion', 'zebu', 'começa', 'dia', '25'], ['safra', 'record', 'disponibilidad', 'crédito', 'ativam', 'venda', 'máquina', 'agrícola'], ['degradação', 'terra', 'mau', 'uso', 'solo', 'avança'], ['desertificação', 'tornou', 'crítica', 'produtividad', '52', 'mil', 'km²', 'região'], ['dia', '15', 'dia', 'conservação', 'solo', 'único', 'fato', 'festejar', 'pode', 'convenção', 'internacion', 'sobr', 'desertificação'], ['produção', 'brasileira', 'pinto', 'cort', 'totaliz', 'fevereiro', 'último', '166', 'milhõ', 'volum', '6', '79', 'superior', 'registrado', 'fevereiro', '93', 'segundo', 'dado', 'associação', 'brasileira', 'produtor', 'pinto', 'cort', 'apinco'], ['apinco', 'destaca', 'boletim', 'mensal', 'setor', 'avícola', 'otimista', 'atual', 'programa', 'estabilização', 'econômica'], ['melhoria', 'padrão', 'aquisitivo', 'resulta', 'maior', 'demanda', 'carn', 'frango'], ['secretário', 'agricultura', 'paulista', 'roberto', 'rodrigu', 'aprov', 'pacot', 'trigo', 'anunciado', 'final', 'março', 'governo', 'feder']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.Conclusão**"
      ],
      "metadata": {
        "id": "nB_C0yVX-f89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "O  presente trabalho realizou uma breve análise exploratória para compreender os dados, a limpeza dos dados, a separação em sentenças, a tokenização, a remoção de stopwords e a comparação da Lemmatization com a Stemming de cada frase.\n",
        "\n",
        "O texto tinha formato de Manchetes jornalísticas, com diversas sujeiras que interferiam em sua clareza. Dessa forma, foi essencial realizar a limpeza dos dados. Foram removidas as Tags html, os erros de concordância nominal que deixa a leitura confusa e sem clareza, a repetição de letras, tal qual \"a a\"(transformou em à), palavras sem sentido para o significado da frase (  A degradação das terras \"avança em o\" - avança onde?, como não tem complemento, foi removido), e remoção de aspas, pois as mesmas não foram abertas nas frases que as continham.\n",
        "\n",
        "Os dados antes da limpeza eram 206, considerando cada palavra, caractere, Tag HTML. E após a limpeza, com o procedimento informado acima, foi reduzido para 167 tokens.\n",
        "\n",
        "A limpeza de dados, portanto, se tornou essencial no processo de identificar e corrigir ou remover dados imprecisos, incompletos, irrelevantes ou duplicados em um conjunto de dados. Esse processo é fundamental em ciência de dados, análise de dados e machine learning, pois a qualidade dos dados impacta diretamente na eficácia das análises e dos modelos.\n",
        "\n",
        "Durante a remoção de stopwords usando a lista da lib NLTK, seria removida a palavra \"são\", o que resultaria em uma frase sem sentido. Portanto, tal palavra foi transformada em SP.\n",
        "\n",
        "Por fim, sobre a Lemmatization e Stemming de cada frase, foi observado que em ambos algoritmos ocorreram falhas. No processo de stemming deve ocorrer a redução de palavras à sua raiz ou forma base, e a remoção de prefixos e sufixos. Todavia, em algumas palavras isso não ocorreu, como em \"ativam\", \"desertificação\" e outros.\n",
        "\n",
        "E a lemmatization reduz palavras à sua forma canônica ou lema, considerando o contexto e a parte do discurso, assim as palavras \"ativam\", \"vendas\" não foram transformadas de forma coerente.\n",
        "\n"
      ],
      "metadata": {
        "id": "OHmPSmO94Ggf"
      }
    }
  ]
}